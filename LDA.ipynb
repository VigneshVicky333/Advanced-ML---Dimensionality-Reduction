{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3978ceb-ab16-4de4-818d-1cf027bbf0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f36e3-849f-4dbc-935d-02041db9e373",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is a dimensionality reduction technique that is particularly used when you have labeled data and want to reduce the dimensionality of your feature space while preserving as much class discriminatory information as possible. Unlike PCA, which is an unsupervised method focusing on variance, LDA is supervised and focuses on maximizing the separation between multiple classes.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Goal:** \n",
    "LDA aims to find a new space that maximizes the separation (or distance) between different classes while minimizing the spread (variance) within each class.\n",
    "\n",
    "**Discriminants:** \n",
    "LDA computes the directions (linear combinations of features) that best separate the classes.\n",
    "\n",
    "### Steps to Apply LDA:\n",
    "\n",
    "1. **Compute the within-class scatter matrix and between-class scatter matrix.**\n",
    "2. **Compute the eigenvalues and eigenvectors of the matrix formed by the inverse of the within-class scatter matrix multiplied by the between-class scatter matrix.**\n",
    "3. **Sort the eigenvalues and choose the top n eigenvectors.**\n",
    "4. **Project the original data onto the new space formed by these n eigenvectors.**\n",
    "\n",
    "### When to Use LDA:\n",
    "\n",
    "- **When the number of observations is greater than the number of features, and you want to reduce dimensionality while maintaining class separability.**\n",
    "- **It works best when you have labeled data with multiple classes.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05b2b8c-44c1-447d-8441-d87a5e8dcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139d2a0d-1bae-4765-b552-957866b41d8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components cannot be larger than min(n_features, n_classes - 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Step 4: Apply LDA to reduce dimensionality\u001b[39;00m\n\u001b[0;32m     34\u001b[0m lda \u001b[38;5;241m=\u001b[39m LDA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# You can change n_components based on your dataset\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m X_lda \u001b[38;5;241m=\u001b[39m \u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Step 5: Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m     38\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_lda, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\discriminant_analysis.py:622\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m max_components:\n\u001b[1;32m--> 622\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    623\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components cannot be larger than min(n_features, n_classes - 1).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    624\u001b[0m         )\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: n_components cannot be larger than min(n_features, n_classes - 1)."
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "dataset = pd.read_csv('prep.csv')\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Handle categorical columns (use LabelEncoder or OneHotEncoding depending on your data)\n",
    "labelencoder = LabelEncoder()\n",
    "# Assuming the target column is the last one (modify if necessary)\n",
    "y = dataset.iloc[:, -1].values  # Target variable\n",
    "X = dataset.iloc[:, :-1].values  # Features\n",
    "\n",
    "# If you have categorical variables in X, apply label encoding\n",
    "# Example: Apply encoding to columns that are categorical, e.g., the second column\n",
    "for i in range(X.shape[1]):\n",
    "    if X[:, i].dtype == 'object':  # Check for categorical columns\n",
    "        X[:, i] = labelencoder.fit_transform(X[:, i])\n",
    "\n",
    "# Step 3: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the features\n",
    "\n",
    "# Step 4: Apply LDA to reduce dimensionality\n",
    "lda = LDA(n_components=2)  # You can change n_components based on your dataset\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Step 5: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Initialize classifiers\n",
    "models = {\n",
    "    'Logistic': LogisticRegression(),\n",
    "    'SVMl': SVC(kernel='linear'),\n",
    "    'SVMnl': SVC(kernel='rbf'),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Navie': GaussianNB(),\n",
    "    'Decision': DecisionTreeClassifier(),\n",
    "    'Random': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Step 7: Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Step 8: Convert results to a DataFrame and print\n",
    "results_df = pd.DataFrame([results], index=['LDA'])\n",
    "print(\"Model Performance with LDA:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043c4ac-9ee7-4535-a91a-9689d1a56a18",
   "metadata": {},
   "source": [
    "The error `ValueError: n_components cannot be larger than min(n_features, n_classes - 1)` occurs because when applying Linear Discriminant Analysis (LDA), the number of components you specify (`n_components`) cannot exceed the minimum of the number of features (`n_features`) or the number of classes minus one (`n_classes - 1`).\n",
    "\n",
    "### Explanation of the Error:\n",
    "LDA is a supervised dimensionality reduction technique, and its maximum number of components is limited by:\n",
    "- The number of features (columns) in the dataset.\n",
    "- The number of classes (unique values) in the target variable `y`.\n",
    "- Specifically, the maximum number of components is `min(n_features, n_classes - 1)`.\n",
    "\n",
    "### Solution:\n",
    "- **Check the number of classes:** If you have a small number of unique classes in your target variable `y`, the maximum possible value for `n_components` will be limited.\n",
    "- **Adjust the number of components:** Set `n_components` to be no greater than `min(n_features, n_classes - 1)`.\n",
    "\n",
    "### Steps to Fix the Issue:\n",
    "1. Check the number of unique classes in `y`.\n",
    "2. Set `n_components` to the appropriate number based on the number of features and classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fb12b-9714-4524-8f5c-acd567c25551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44215091-3247-4c4e-8fb7-6580b0f197d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 24\n",
      "Number of classes: 2\n",
      "Maximum components for LDA: 1\n",
      "Model Performance with LDA:\n",
      "     Logistic   SVMl  SVMnl    KNN  Navie  Decision  Random\n",
      "LDA     0.975  0.975  0.975  0.975  0.975    0.9625  0.9625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "dataset = pd.read_csv('prep.csv')\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "labelencoder = LabelEncoder()\n",
    "y = dataset.iloc[:, -1].values  # Target variable\n",
    "X = dataset.iloc[:, :-1].values  # Features\n",
    "\n",
    "# Encode categorical variables if needed\n",
    "for i in range(X.shape[1]):\n",
    "    if X[:, i].dtype == 'object':  # Check for categorical columns\n",
    "        X[:, i] = labelencoder.fit_transform(X[:, i])\n",
    "\n",
    "# Step 3: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the features\n",
    "\n",
    "# Step 4: Check number of classes and features\n",
    "n_classes = len(set(y))\n",
    "n_features = X_scaled.shape[1]\n",
    "max_components = min(n_features, n_classes - 1)\n",
    "\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Maximum components for LDA: {max_components}\")\n",
    "\n",
    "# Step 5: Apply LDA to reduce dimensionality\n",
    "lda = LDA(n_components=max_components)  # Use the calculated max_components\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Step 6: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Initialize classifiers\n",
    "models = {\n",
    "    'Logistic': LogisticRegression(),\n",
    "    'SVMl': SVC(kernel='linear'),\n",
    "    'SVMnl': SVC(kernel='rbf'),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Navie': GaussianNB(),\n",
    "    'Decision': DecisionTreeClassifier(),\n",
    "    'Random': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Step 8: Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Step 9: Convert results to a DataFrame and print\n",
    "results_df = pd.DataFrame([results], index=['LDA'])\n",
    "print(\"Model Performance with LDA:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52414e-2824-47ea-a407-77689a9df166",
   "metadata": {},
   "source": [
    "| Number of features         | 24   |\n",
    "|----------------------------|------|\n",
    "| Number of classes          | 2    |\n",
    "| Maximum components for LDA | 1    |\n",
    "\n",
    "## Model Performance with LDA:\n",
    "\n",
    "| Model     | Logistic | SVM (Linear) | SVM (Non-linear) | KNN    | Naive Bayes | Decision Tree | Random Forest |\n",
    "|-----------|----------|--------------|------------------|--------|-------------|---------------|---------------|\n",
    "| **LDA**   | 0.975    | 0.975        | 0.975            | 0.975  | 0.975       | 0.9625        | 0.9625        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c87c41-3f59-41c2-9fdc-5b04211d8a3d",
   "metadata": {},
   "source": [
    "## Key Changes:\n",
    "- Determine the maximum number of components (`max_components`) based on `min(n_features, n_classes - 1)`.\n",
    "- Set `n_components` dynamically based on the above calculation.\n",
    "\n",
    "### Example Output:\n",
    "If your dataset has:\n",
    "- 10 features.\n",
    "- 3 classes (i.e., the target variable `y` has 3 unique values), then the maximum number of components for LDA will be `min(10, 3-1) = 2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17767e4f-8ae3-44b5-bd18-e8c5098de56c",
   "metadata": {},
   "source": [
    "## Important Points:\n",
    "- If `n_classes - 1` is smaller than `n_features`, you'll need to use that as the upper limit for `n_components`.\n",
    "- The accuracy of the models may vary depending on the dataset and the transformations applied (such as LDA).\n",
    "- This should resolve the issue you're encountering and allow you to proceed with LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba23ec-5950-46af-a1b7-cf0103c3d48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
